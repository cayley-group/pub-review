
RECENT ADVANCES

Protein folding - Drori et al. (2019)

Evolutionary information can be used to build a pairwise relatedness matrix between all pairs of elements of a protein sequence. This is based on the aforementioned tendency for structurally-related sequences to be correlated in their change over the course of evolution. It is possible to compute such a distance matrix directly from these correlations (for example, as the result of a multiple sequence alignment or MSA) as well as to predict the matrix using a neural network from the same input data - for example, given a library of known input sequence to distance matrix mappings.

In the latter case, the intermediate step of embedding the input data is relevant. That is, given an MSA or other auxiliary data input, that data must be represented in “tensor” (basically a higher-dimensional matrix) form. The process of mapping from raw input data into a high-dimensional representation space is referred to as “embedding”. Drori et al. (2019), who attained competitive results with AF in CASP13, computed these embeddings in the course of training a larger end-to-end system (as opposed to doing so in a separate initial step as is sometimes helpful for training language models). Representations were computed by passing each of four different types of feature through neural network layers to produce intermediate representations that were subsequently concatenated either (1) before application of the main encoder module or (2) after applying the main encoder module to each of the four input streams. Input feature types included (1) the amino acid sequence, (2) n x 21 PSSM data produced from HHBlits, (3) an n x n MSA covariance map produced by jackHMMER, and (4) n x 8 secondary structure data produced by DSSP.

In Drori et al. (2019) intermediate representations are then passed to two different decoders - one to predict a map of pairwise backbone distances and another to predict a map of bond torsion angles. Networks are trained simply by optimizing their parameters to increase the similarity between predicted and ground-truth outputs. Subsequently in Drori et al. (2019), 3D coordinates are predicted for the full backbone to satisfy the predicted pairwise distances - minimizing a loss written as the sum of squares of difference between predicted distances and those given by the current 3D conformation. Following a 3D prediction for the backbone, predictions for the rest of the protein’s atoms are initiated using predicted torsion angles and solved through optimization to satisfy a function that describes the extent to which an existing conformation is expected according to the laws of chemistry.

Protein folding - Senior et al. (2019)

The winning method for CASP13 is presented in Senior et al. (2019). This method chiefly involves the prediction of a residue pairwise distance map that is subsequently used as a guide for 3D structures by gradient descent. Distance prediction models are trained by comparing distance predictions to those extracted from databases of solved structures.

The input to the distance prediction network was a tensor of shape L x L x F where L is the length of the amino acid sequence and F is the number of features - that being, for every pair of (i, j) in L x L, one scalar value of the MSA covariance of residues i and j, over 100 sequence-level features for each of i and j, and hundreds of pairwise sequence-level features - all of which were concatenated to form one feature vector of length F (in contrast to e.g. Drori et al. (2019) that operates on distinct feature streams with distinct embedding networks).

The quality of MSA data is core to the approach for reasons discussed above. Naturally, they indicate that a key factor in the accuracy of predicted distances is the number of non-redundant sequences in the MSA. Further, greater use of a fixed size MSA dataset was attained using data augmentation whereby MSA’s were sub-sampled with noise and dropout.

To this input was applied a dilated convolutional ResNet architecture with everywhere two-dimensional representations (i.e. without one-dimensional embedding bottlenecks) that was architected to produce a pairwise distance map. Distance maps are not ones of single distances but rather a discrete distribution of distances for each pair over 64 bins across the range of 2 to 22 angstroms. For example, amino acids 2 and 6 might be predicted to have an expected distance of 13 angstroms - therefore have the largest value in the 13th bin, next largest values in the neighboring 12th and 14th bins, and so on - with the spread of this distribution reflecting the confidence of the 13 angstrom prediction. Interestingly, ablation analysis indicates model performance hardly degrades as the number of bins is decreased from 64 down to around 6 suggesting training speed and memory requirements may improve substantially with such a modification.

Models were trained by comparing patches of the predicted distance map to the experimentally-derived distances (to which noise was added for data augmentation). Comparing off-diagonal patches instead of whole images was beneficial for data augmentation purposes, to enable the emphasis and control of importance of distant predictions, and to increase the speed of training. Also included in the training loss were auxiliary loss terms scoring secondary structure and accessible surface area which was reduced in importance by a factor of 10 after 100,000 steps. The authors report the use of early stopping and model selection from a pool of trained models using two different validation datasets distinct from the training dataset. These authors have reported previously the benefits of population-based tuning methods which would be consistent with this paradigm - wherein one does not simply train a flat population of model variants but rather proppogates favorable hyperparameters or model parameters onwards to successive generations of training. They report a single training round consumed only 8 GPUs over the course of 5 days but one could expect their having performed hundreds of such rounds across either a flat or population-based tuning strategy. Further, they report best performance coming from the ensembling of four different models.

In addition to distances, models also predict bond torsion angles. Given these data, any folded polymer structure can be scored according to its congruence with these - in domain-specific language, an energy function whose lowest-energy form is sought. Discrete distance and bond angle distributions are rendered differentiable by fitting smooth functions to them and subsequently minima are sought via gradient descent according to the L-BFGS algorithm. Each optimization round leads to a local optimal solution given an initial structure sampled in accordance with distances, angles, and MSA data. To find the global optimum or lowest-energy conformation, initial conditions are repeatedly re-sampled and optimization is repeated to search for better and better solutions including re-starting previously-used initial conditions with noise - another method reminiscent of population-based tuning.

Resulting structures were compared to ground truth structures by computing a TM score that compared the similarity of the backbones of the two structures.

Protein folding - Yang et al. (2020)

At the date of its publication, Yang et al. (2020) was state of the art and improved upon the Senior et al. (2019) result that won CASP13. Their approach also used evolutionary information and other features as input to a neural network to predict a 2d map of distance and angle optimization constraints for 3d solving using gradient-based optimization methods.

Input MSA data was derived from a variety of data repositories including Uniclust30, JGI metagenomes, JGI metatranscriptomes, JGI eukaryotes, UniRef100, NCBI TSA, and 2,815 additional genomes collected from additional depositories. MSAs were used to construct a 441 dimensional pairwise coupling feature and one correlation feature for each residue pair. These further include one-dimensional features for each amino acid sequence position (including position specific frequency matrices and a positional entropy feature).  As in Senior et al. (2019) all input features for pair (i, j) were concatenated in the third dimension of an LxL input matrix for sequences of length L. Data augmentation was performed by randomly sub-sampling MSAs at training time, as in Senior et al. (2019), which conferred a 2.2% improvement over baseline.

Notably, the authors report the single largest improvement over baseline for a genomic analysis pre-processing step, that being 3.1% for MSA “alignment selection”. They make reference to “... further improv[ing] the results by feeding a variety of MSAs generated with different e-value cutoffs or originating from searches against different databases, into the network and selecting the one that generates predictions with the highest predicted accuracy”. The database selection and e-values yielding the highest accuracy appears not to have been reported. Presumably further curation and selection could be performed to prevent spurious signals of co-evolution by discounting according to the overall evolutionary relatedness of the genomes from which sequences are derived instead of applying a hard filter against elevated homology.

The neural network architecture used is similar to others - a deep convolutional residual network that forks to produce multiple output maps including a distance matrix and three angle matrices (the inclusion of which conferred a 1.7% improvement over baseline). Similar to Senior et al. (2019), predicted distance and angle matrices were discrete distributions instead of single values (the range 2 to 20 angstroms over 36 bins plus one to indicate non-contact for distances; fewer for orientation maps). The authors report training only five networks and attaining improved predictions by, as in Senior et al. (2019), ensembling over these. Each network takes 9d to train on a single GPU (no GPU or other accelerator parallelization was reported). 

Further as in Senior et al. (2019), model predictions are then used to parameterize the energy function that is used as the objective in optimizing for lowest-energy structures; further constraint matrices are also smoothed to enable solving with gradient-based methods similar to L-BFGS. (The optimization model is made available via trRosetta.) A diversity of starting structures is maintained including by randomly perturbing bond angles. Prior to solving, overly low or high-confidence angle or distance constraints are discarded and relevant constraints are included progressively by distance (nearest first). From a diversity of initial conditions and optimization parameter configurations the lowest-energy backbone structure solutions are selected and for these a final round of solving is performed for the full atomic structure using Rosetta FastRelax.

Graph neural networks

Biological polymers can be perhaps best represented as a graph over the individual atoms (that is, where each atom constitutes a node in the graph and each covalent bond constitutes an edge). Likewise, hydrogen bonds arising from the interaction of these polymers with themselves or other polymers (or e.g. ligands) can also be represented as edges in such a graph (albeit denoted differently to reflect the difference in bond type. Further, the electron density distribution of such polymers can also be modeled as a graph together with a charge density map over that mesh (exactly as we are accustomed to viewing volumetric or charge density visualizations of molecules, e.g. Figure 2). Machine learning can often be enhanced in its performance by choosing models and representations that mirror their subject matter closely in terms of geometry or other real aspects of how the thing that is sought to be represented works.

Graph neural networks present such an opportunity in the context of learning to fold and simulate biological polymers. Quite simply, each node in such a network is a learned function that receives and sends information to its neighbors while maintaining a latent state representation.

Mesh graph neural networks - Pfaff et al. (2021)

Recently, state-of-the-art advances in physics simulation using graph neural networks have been attained by Pfaff et al. (2021). These results are notable for their attaining the above discussed requirements of high accuracy, 1-2 orders of magnitude of speed improvement, and generalization in space and time permitting training on small simulations and scaling to large ones at inference time. This result is attained using a hybrid approach wherein the simulated objects are converted to mesh (as opposed to grid or particle) representations that are subsequently updated by the GNN. In short, such approaches simply update a feature vector at each node of a graph representation by repeated rounds of message passing between graph neighbors. This learning paradigm can be related to the way physical systems occur in the real world, indeed by propagating their dynamics through local neighborhoods of space - the graph network thus being a very near analogy in architecture to the geometry of the physical problem being modeled. Further, the granularity of the mesh is adaptively re-computed and updated so as to prioritize simulation detail where it is most needed (e.g. at the folds of a cloth simulation) - according to an energy function describing this. The authors speculate that the irregularity of simulation scale incurred by such adaptive and changing representation scales functions as a sort of regularization permitting the learning of a model that is not narrowly applicable to a single simulation scale.

Inspired by this it would be interesting to explore whether mesh randomization, as opposed to intelligent re-meshing, would provide the same kind of generalization benefit with a significant improvement in computational efficiency. Further it would be interesting to explore the best way the space-filling electron density map of a protein can best be represented with such a mesh representation (e.g. refer to Figure 2 for an intuitive starting point).

Figure 2. Backbone versus atomic-level space-filling model.

January 2020, David Goodsell
doi:10.2210/rcsb_pdb/mom_2020_1


Learned optimizers

Not all optimizers are equivalent in their tendency to discover global minima. Nor is any one optimizer capable of being optimal for all optimization problems. Of relevance to our subsequent discussion is recent work in the use of neural networks to learn optimization algorithms that are specialized to (i.e. perform better on) the optimization problem on which they are trained. In the work of Senior et al. (2019), Yang et al. (2020), and others, static optimization constraints are predicted then provided to a static, general purpose optimization algorithm (with some amount of manual tuning of optimization parameters to the specific task). Andrychowicz et al. (2016) demonstrate the design of a domain-specific optimization algorithm that subsequently was shown to out-perform all of the best in class optimizers available for that problem (that had been a legacy of painstaking effort over more than a decade).

AlphaFold2

Recently a group from Google competed in and won CASP14 achieving prediction accuracy nearly on par with experimental results for crystallography-determined structures. A paper has yet to be published reporting the method that was used and details are spotty other than a suggestion in one of their blog posts as to the relevance of thinking of a protein structure as a spatial graph. Details sufficient to reproduce the method have been promised to be made available at some point in the future but are not currently available. One interesting claim was that their model can assign a measure of confidence to its predictions permitting better model selection - that also being an indication of the use of model population and selection approaches in searching over model variants. For example, two models may make similar predictions but the model that knows where it is wrong is more useful and probably more apt to smoothly improve in that regard in the future than one that is just blindly wrong to an unknown extent.