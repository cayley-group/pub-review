TECHNOLOGY CONTEXT

A solution to the biophysical simulation problem may be within reach sooner than one would intuit due to exponential progress in various domains including artificial intelligence, computing, and genome sequencing together with hard-to-quantify rates of progress in domains such as automation of processes in the biological sciences through miniaturization. Here we explore each in series towards further expanding the intuition of the reader about the current technology context.

Artificial intelligence

Perhaps the most exciting and popular among the technology topics is that of artificial intelligence. In the course of training an AI algorithm we specify the desired behavior implicitly according to what examples are provided and how success is measured. The algorithm then automatically learns to identify features in the input and construct its own byzantine dynamics to fulfill the desired input/output mapping (with performance scored according to that formal measure of success).

In recent years, brilliant advances have been made in the use of neural networks for the simulation of physical phenomena including those of macroscopic fluids, solutions to complex many-body quantum wave equations, acoustic systems, and so on. Typically, but not always, these are trained using an existing means of simulation that produces simulation time-series on which the neural network is trained (e.g. to produce forward predictions concurrent with the behavior of the simulator). One of the primary benefits of this has been to enable the acceleration of such simulations by between one and five or more orders of magnitude - thereby beginning to open the door to new applications that were otherwise infeasible.

To be specific, one might imagine (1) simulating the fluid dynamics inside a microfluidic chip such as areas of turbulent flow or phenomena of droplet shearing, (2) learning a neural network to perform such simulations very rapidly, then (3) searching over the space of fluidic chip designs to achieve a desired distribution of droplet sizes.

Not all physical phenomena can be modeled by us, currently. We may have the means to model parts of a system but lack the means to fully account for the sophisticated dynamics of subsystems interacting at their boundaries. Or we may be able to simulate dynamics under idealized conditions but not be able to simulate realistic conditions accurately enough to obtain useful results.

Another exciting benefit of learned simulators is that models can be learned directly from experimental data - potentially overcoming the previous challenge. As in the previous example, suppose *for the sake of argument* that we are unable to prepare models to effectively simulate the behavior of our microfluidic chip with sufficient accuracy provided any amount of time and computational resources. To overcome this, we could prepare video recordings of the flow of droplets through many of a diversity of chip designs then train our neural network to simply emulate the real dynamics of droplet flow instead of emulating that produced by our simulator. Here there is the potential to exceed the performance of a simulator-trained model because the training data truly contains within it the dynamics we hope to emulate whereas, if trained on incomplete simulated data, the best the model could perform would be similarly incomplete simulations.

The generalizability of a learned model is top of mind of any AI researcher. When a model is trained on one dataset it may perform well when presented examples that are of a familiar type but fail when unusual ones are seen. Models and the training strategies used to train them differ in their generalizability - indeed some with such that translates into their being highly useful.

In the case of physical simulation, there is the need to generalize to systems and conditions of interest that do not match those that were present at training time. Relatedly, the ability to generalize from small to large simulations means models can be trained more quickly on smaller examples then be scaled up to perform (spatially) larger simulation at inference time. Further relatedly, the ability of a simulator to generalize to temporally larger simulations is also useful - allowing models to be trained on short example simulations then generalize to perform potentially very long ones. Another way to refer to this latter property is that the simulation remains stable under long simulation roll-outs - avoiding the chaotic accumulation of artifacts and erroneous dynamics that eventually occur in simulators that are not stable in this way.

One fine point of the issue of scalability is whether scaling is linear or nonlinear. In linear scaling, increasing the size of the simulation by a factor of n always results in an increase in the running time or memory other resource requirements to increase by a constant-valued multiple of n. That is, the function f(n) that maps from the size of the input to resource requirements is a linear function of n (e.g. f(n) = k*n for some constant k, such as 1, ½, 10, 0.1234, ...). When scaling is adversely non-linear, a colloquial way to describe the situation as one of “diminishing returns” or that of a “ceiling effect” - whereby, beyond a certain scale, adding more compute resources may not provide a cost-effective benefit. For example, algorithms of exponential complexity may e.g. scale according to the function f(n) = knm for constant values k and m. One can understand that when m is greater than one, such as 2 or more, the tradeoff between resources and benefit will resemble a parabola and clearly make it resource-impossible to reach a certain regime of input sizes (indeed in either space or time). Attention is paid here to this issue because, as we will discuss later, many biophysical systems of practical relevance are very large and involve the dynamic behavior of not only large multimers but systems of these.

To summarize, neural networks can be used to learn to simulate physical phenomena from either simulated or real examples. In doing so, speed and accuracy benefits can be attained. Key challenges include the generalization of learned models beyond the training context both in regard to system design and conditions as well as system scale both physically and temporally. And the function that describes the scaling tells us about the limits (or non-limits) on the scale of simulations that could be performed.

Large-scale computing

A natural continuation of the previous discussion, modern advances in computing create a historically unique opportunity to solve certain scientific problems. Scientific computing can thank various consumer industries for fueling Moore’s law for several decades - including recently the gaming industry for fueling the advancement in GPU processors that made possible many of the advances of the first two decades of this century. As the value proposition for AI has become more clear, investment has increased in processors custom-made to perform certain tasks that are central to many neural network approaches.

Likewise, the AI field can thank other industries in the growth in sophistication of cloud computing offerings. Today, not only does the cloud offer latest-generation AI chips to rent (at a rate of a couple of dollars per hour) but it offers these in further rentable clusters of dozens or hundreds of such accelerators, interconnected with optical communication and achieving such high marks on availability and reliability that are only possible through the power of free-market capitalism (for better or worse). Providers compete to provide faster accelerators, better documentation, and greater reliability and availability while funding the improvement of their services with the reinvestment of revenue.

Automation and miniaturization in the biological sciences

Automation has made possible certain scale and precision in the biological sciences that would not be feasible otherwise. Both robotics and the miniaturization of laboratory processes with microfluidics are complementary routes to such automation and scale. Microfluidic chips are at the heart of all modern DNA sequencing platforms as well as many essential laboratory instruments. Microfluidic chips can be made to miniaturize PCR, for example, by flowing droplets through regions of higher and lower temperature - permitting it to be performed at a lower cost, higher scale, or in settings not permitting commercial PCR machines such as consumer smartphone-attached diagnostic devices.

Genome sequencing and assembly

The genome sequencing revolution has began with the sequencing of the human genome around the turn of the century with exponential advances in sequencing technology permitting genome sequencing projects to progressively scale from a few model organisms or those of key economic importance to an effort to sequence and assemble the genome of every species on Earth. These datasets have the potential to shed broad new light on evolutionary history.

Today, protein folding, like supposedly all things, only makes sense in the light of evolution. The selective pressure of natural selection is applied to organisms manifesting the folded three-dimensional structures of their proteins. When a mutation occurs that affects this three-dimensional structure in a beneficial way, more copies are propagated. Thus we can examine an evolutionary history to make some inferences about what regions of a protein's sequence likely occur nearby in three-dimensional space.



Figure 1. Illustrative evolutionary conservation information UCSC Genome Browser. (Kent et al., 2002)

Relatedly, for illustration, one can imagine a receptor protein positioned within the membrane of a neuron - just at the boundary of a synaptic junction across which neurotransmitters are communicated. Such a receptor has two main parts - one for detecting the incoming message and the other for performing some function in the receiving cell. In short, when the right thing is detected, the shape of the receptor protein changes so as to trigger the interior function - much like pushing in the wings of an origami crane causes its head to lift with pride. Throughout the course of evolution, as the need for complexity in the representativeness of nervous systems increased, so did the diversity of signaling mechanisms - permitting the representation of greater and greater complexity with the same amount of space and energy. But to enable this, evolution did not need to re-invent the neurotransmitter receptor paradigm for each new type or regulatory condition of neurotransmitters. A paradigm known as “duplicate and diverge” was naturally applied. As in this example, as duplicated regions of the genome encoding an existing receptor naturally occur, mutations accumulated in a duplicated copy may come to permit the recognition of new kinds of neurotransmitters. 

From our modern perspective, when comparing the evolutionary history of sequences, the observation that there is one region of a protein that has ancient evolutionary roots whereas another that is as recent as the emergence of modern humans we might naturally infer that either of these two regions are nearby themselves in three-dimensional space (in this case, in the synaptic junction vs. inside the cell). One can imagine the same reasoning can be applied in a more or less fractal pattern - with increasingly high-resolution evolutionary information providing increasingly high-resolution insight into the folded structure of proteins.
