

PATHS FORWARD

End-to-end training and newly differentiable components

As discussed in the section on learned optimizers and reflecting on the structure solving steps of Senior et al. (2019) and Yang et al. (2020) one might imagine the possibility of learning a neural network structure solver as a part of the larger end-to-end trainable system. Here, instead of learning to faithfully simulate all of the physics of the polymer and the natural relaxation of the structure into its folded form - an optimization well understood to be beyond resource-possible - the simulator learned would be more so learning to function as a learned constrained optimization solver. Whereas previously those constraints were provided to hand-crafted constrained optimizers as explicit values for expected distances and angles here what would be provided to the learned optimizer would be latent (optimizable) representations. The simulator would be learning to relax an input polymer sequence in a fashion that is both consistent with the laws of physics as well as the provided constraints and could further tailor the learned algorithm to aspects of the particular optimization problem.

One thing this sort of end-to-end learning would accomplish would be to permit signals of difficulty in the 3D structure solving task to be propagated backwards to earlier portions of the system permitting their representational capabilities to be enhanced specifically in the service of this primary downstream task. Besides the improvement of networks that learn to represent pairwise distances this might further include the embedding or other networks used to represent evolutionary information (see below section on improved MSA representations). In short, Yang et al. (2020) and Senior et al. (2019) both demonstrated significant improvements to be derivable from improvements in the way evolutionary history information was represented. Permitting this portion of the network to also be learnable could permit much further gains beyond the hand-crafting that has thus far provided these results - in keeping with the broader history of the way deep neural networks have shown such great success by automating the selection and modeling of features.

Forms of pre-training and learning curricula

Naturally it would be unnecessary or impractical to train such a system end-to-end from scratch. Various forms of pre-training for subcomponents are plausible. For example, a distance prediction network could be trained before and/or alongside a downstream structure solving network. Such an approach would be similar in spirit to previous approaches that have shown good results - i.e. by learning to directly predict pairwise distances and compare those to those known by way of already solved structures. Once good performance had been attained on this task it would be far more feasible to feed the representations of the top of the distance-predicting network through to one capable of learning to solve 3D structures.

The 3D structure solving module could itself be pre-trained in much the same way as in Pfaff et al. (2021) to produce coherent, scalable, and time-stable simulations of polymer-like objects such as the cloth simulations they reported in their manuscript. Although seemingly far from similar to the motion of biophysical polymers a simulator capable of simulating a long thin flag that maintains its linear structure, avoids diverging into chaotic artifactual messes, and accurately models rigidity in situations of self-intersection - all would be a long running head-start to learning from the comparatively scarce amount of protein structure data available. Having been pre-trained in this way this module could be further pre-trained to predict the simulation traces of polymer or electron density simulations. That is, before being fine-tuned on the structure optimization task that is greatly simplified through the inclusion of evolutionary data.

Another learning curricula approach may be a progressive back-off of fully solved structures - i.e. initially introducing a small amount of noise into a fully-solved structure. As the ability of the model to predict the final structure improves more and more “noise” or other forms of back-off can be added to increase the difficulty of the problem in a progressive fashion. Noise could be added selectively at the level of secondary or tertiary structure.

We might further consider training models to predict secondary structure before predicting tertiary structure as the former is to some extent a sub-problem and at least useful knowledge for the latter. One might generalize this learning curricula to relax a weighting of the importance of distance matrix predictions by distance over the course of training - i.e. continuously weighting the importance of predictions at all distances starting with high importance for close distances (i.e. secondary structure) and low importance for long distances with intermediate and long-distance importance increasing through the course of training. Alternatively, all distances may be given the same importance but distance matrix patch samples of closer regions may be obtained at higher frequency than distant ones earlier in training (with the potential to smoothly adapt the sampling as a function of the model’s performance).

Stand-alone embedding of genetics information

Another form of pre-training that could be performed would be to learn and subsequently optimize embeddings of protein sequences - analogous to methods of language modeling used as pre-training for many kinds of language models. Methods used to learn such representations have included autoencoder and denoising autoencoder approaches, context prediction methods (e.g. given word, predict which words are likely to occur before or after it given a large collection of sentences, thereby learning representations that place related words nearby in a learned representation space), forward prediction methods, and co-occurrence methods - many of which fall under the heading of “self-supervised learning”.

With an eye of intuition turned towards the duplicate and diverge paradigm of evolution, an approach that learns to embed kmers in a way such that those that tend to occur together have a closer position in embedding space. For example, quickly searchable indices of all known sequences could be constructed and queried for pairs of kmers sampled from an input protein sequence. An embedding model could then learn to predict the frequency with which these kmers occurred together throughout the databases. Either kmer searches and such similarities could be queried against databases on the fly or distances for all pairs of kmers could be pre-computed and stored in a rapidly queryable key-value store like Google Cloud BigTable. There are approximately 38 billion pairs of 4mers of the 21 amino acids, an amount storable in the RAM of some machines but representations of these at 5mers and beyond would consume over 16TB of memory - the most that is offered on the Google Cloud is 12TB and it is unclear whether single machine or manual serving of data to training workers would plausibly provide any benefit over the highly optimized BigTable offering that is commonly used for this purpose.

A two-dimensional matrix of amino acid kmers (from a query AA sequence) by species could also be provided to networks that are able to learn to most effectively extract from those relevant signals of correlation - i.e. instead of directly computing a single figure from a formula with a variety of hyperparameters to tune such as the e-value cutoffs of Yang et al. (2020). For example, such learned processing might be a form of correction for over-representation of certain parts of the evolutionary tree otherwise leading to fallaciously elevated signals of evolutionary covariance. Alternatively, providing a listing of the occurrence of certain kmers through an evolutionary history (instead of collapsed to a single covariance figure) gives networks additional opportunity to learn to extract useful information about the history of how a certain kmer arose.

Regardless of the means of embedding, key points were that the networks for embedding can be learned independently as pre-training then can continue to be optimized through subsequent end-to-end training. A key objective in learning these embeddings is to emphasize the evolutionary relatedness of regions of the input sequence but permit flexibility in learning exactly how this information is extracted.

Further, we may consider it useful to make use of latest-generation Transformer or other attention-based models that have shown phenomenally good performance on language modeling tasks and are capable of scaling linearly with the size of input - opening up a broader set of opportunities for self-supervised pre-training (given larger input) such as operating on dozens of concatenated gene or protein sequences and/or considering larger regions of the genome at a time. One reason the latter might be useful is to consider evolutionary relationships between genome sections that contain multiple genes that may have evolved to reside in close proximity in the genome for a functional reason - such as their occurring together in a multimer complex.

Better trees and ancestral reconstructions

Towards further mitigating false over-representation of sequences, besides learned means of correction (such as through attention mechanisms), we may seek to draw upon or further develop carefully curated databases of evolutionary sequence trees and ancestral genome reconstructions.

Additional sources of evolutionary information

One of the key advances of recent years in the science of protein structure prediction has been making use of evolutionary information. Going forward, we might either design ever better ways to make use of such information or we might seek better ways to generate such information to begin with. For example, by placing many parallel microbial communities under mutagenic stress and sequencing the evolutionary results we might be able to manufacture a setting in which signals of evolutionary covariance could be obtained with greater resolution. This might include synthetic microbial lines that have engineered into them metabolic pathways of interest and, given the concern of a single user/lab/customer seeking to optimize a single protein or pathway, a structure solver tailored to the classes of structure common to that pathway may not be excessively domain-specific.

Naturally such evolutionary information may simply already be present in the natural diversity of microbial communities that have undergone drift and diversification over millions of years and need only be sampled - such as with long-read sequencing - and need not necessarily be de-convoluted or assembled. This is likely part of the inspiration for Yang et al. (2020) including metagenomic databases when performing MSA searches.

Pre-train on a database of small crystal structures and generalize to larger ones (for the sake of argument)

For the sake of argument, and although it is outside of the expertise of this author, it has been reported that proteins of lower molecular weight (i.e. smaller and shorter ones) are easier to crystallize. For example, single-domain proteins are easier to crystallize than multi-domain proteins. Inspired by the potential of the physics simulation models of Pfaff et al. (2021) to strongly generalize from smaller to much larger simulations - one might seek to use high-throughput microfluidic and/or robotic methods to construct a database of crystal structures of protein fragments (such as sampled from those known to occur in nature). Of course this undertaking would be very expensive but it is plausible given the results of Pfaff et al. (2021) that the means to precisely solve small structures would generalize to larger ones.

This section was included for the sake of argument and as a possible explanation for why the AlphaFold2 algorithm tends to only approximate experimental levels of accuracy for crystallographically-derived structures. With such an approach, while understandable and not immediately obvious and with some amount of benefit of hindsight, there may be a problem generalizing beyond the selection bias that has led to the set of proteins that are feasibly crystallizable coming to constitute training datasets. Further, as constructing this sort of training dataset would be very resource-intensive one would need to be able to somehow test the hypothesis that the ability to solve small polymer relaxations would generalize to larger ones (i.e. that the Pfaff et al. 2021 results in this regard do indeed actually apply). Further, the usefulness of a simulator for this purpose would be limited to the solving of structures and would not necessarily be anticipated to be able to make predictions about the dynamics of individual or multimer structures when encountering other structures or ligands.

Without discounting this class of approaches, one might ask how we might accomplish a similar goal (i.e. build up a training dataset, including perhaps one of small structures that can be used as pre-training for the determination of larger ones) without the expense or technical complexity of crystallography.

Holography methods

The revolution in next-generation sequencing has, among other things, rendered a portion of the biological world directly readable. Historically, as discussed above, this has been notable in terms of the advances in genome assembly and genotyping it has permitted. Over the years it has further enabled the assay of non-DNA elements such as the location of regulatory elements and chromatin structure through specialized molecular biology techniques that encode the desired information in a form that is readable by a DNA sequencer. For example, by binding regulatory elements to the DNA to which they are bound, purifying the elements of interest, then separating the bound DNA from those elements and sequencing it one can obtain a set of regions across an entire genome where a specific regulatory element tends to bind (indeed obtaining a distribution over sites as opposed to a binary indication). The last twenty years in molecular biology is rich with methodological innovations of this sort - that have made use of the readability of DNA to assay other features through methods that encode the desired information in data read by a sequencer.

Prior to the advent of modern AI methods, effectively all of these methods were designed with the intention of applying a hand-crafted method of de-convoluting the result. That is, the desired information was encoded in sequenced fragments in a way that the designers knew how to extract. For example, with the regulatory element binding example, this decoding was as simple as aligning sequenced fragments to a reference genome and identifying peaks in coverage.

One of the benefits of modern AI methods has been to solve problems where we cannot feasibly specify an algorithm to perform a task. For example, it is very difficult to write a hand-crafted algorithm to identify cats in photos although the information is obviously present and at least decodable by us. Deep learning has been able to automate the process of identifying and modeling features in such data domains where the necessary information is definitely encoded but for which a deconvolution algorithm cannot be readily specified by us.

The availability of such deconvolution methods not only creates the opportunity to solve existing problems of these kinds but furter presents an opportunity for molecular biology in the design of new methods that are designed with this flexibility in mind - i.e. that the method need only encode the necessary information in a manner such an algorithm can later learn to deconvolute as opposed to the subset of methods for which we know how to specify these deconvolution algorithms. Thus the relaxed/expanded method design paradigm is to encode the relevant information *somehow* in a form that can be sequenced and learn a model to extract the desired information.

As stated above, a goal would be to train an optimizer (that includes evolutionary information) end-to-end to produce solved 3D structures as well as to identify novel sources of data either at the front end (evolutionary) or back end (ground truth structure data). This may be to the end of improving the means of networks to predict pairwise distances or further to parameterize a physics/optimization model that can solve full structures. Historically, crystallography data has been used to beneficial effect for the latter and could be generated at a larger scale albeit at great expense.

Here we propose another possibility for the means of generating such ground-truth data to supplement or replace crystallography data and term this class of method, in keeping with the above discussion, “holography methods”. A conventional hologram is a two-dimensional surface into which has been encoded a three-dimensional image such that our eyes can extract from the two-dimensional representation and three-dimensional one. So, for the purpose of training a biophysical simulator or specifically a protein structure solver, how might we construct a training dataset that encodes an abundance of three-dimensional structure (if not also kinetic) information in sequence or other affinity assay form? That is, how can we capture the general manner in which biopolymers tend to behave in naturalistic environments in a form that can be sequenced or otherwise through physical assays?

SELEX Holography

The SELEX method involves generating a large diversity of DNA sequences and presenting these to a binding target (e.g. a protein or cell fixed to the surface of a microfluidic chip) after which un-bound oligos are discarded. Bound oligos are then released, amplified, and the process is repeated. This leads to a population of oligos with greater and greater structural affinity for the presented target(s). 

Imagine that we performed a single cell SELEX experiment that led to a frequency distribution over sequences (si) such as is illustrated in Figure 2. Let Scell denote the set of sequences obtained from this cell SELEX experiment. 


Figure 3. Hypothetical frequency distribution over oligos following a single cell SELEX experiment.

As applied traditionally, the goal would be to examine the highest affinity sequences towards designing, for example, cell-specific probes. Here, we are interested in the full spectrum as a signal of the structures that are present. In the course of performing a cell SELEX experiment, oligos will be selected that bind to any of the structures presented on the cell surface. Thus the distribution in Figure 3 is to some extent an assay of all of the geometries of all of the structures on the cell surface - albeit encoded in a form that is perhaps impossible to decode.

Now suppose the same pool of oligos that were obtained through such an experiment (Scell) were applied in a single round of binding to either of single protein targets A or B, with the result being sequenced. One might expect to observe something that in spirit resembles a mass spectrum distribution (Figure 4) wherein differences in the structure of targets A and B is reflected in differences in each frequency distribution over Scell. We might further, for the sake of convenience, refer to these as the “structural holograms” (or perhaps more casually, “barcodes” or “structural spectra”) of A and B in the context of Scell (denoted hAS , hBS for holograms of A and B over sequence collection S).


Figure 4. Hypothetical frequency distributions over Scell for targets A (orange) and B (teal).

Naturally, the diversity of Scell relates to its informativeness in such a task so the means of constructing it is important. We might imagine combining the results of many experiments (whether by this method or another) into one ensemble. Here we will refer to instances of such ensemble combinations as S*. We might further seek to combine not simply the results of cell SELEX experiments but further those performed on cell lysate.

Further consider a set of targets T such that each element is a single protein or fragment of a protein. We can denote the set of all individual target holograms as HTS consisting of elements {hSt| t is in T}. This might be described as the “full structural hologram of targets T over structure probes S” or simply “the full hologram of T over S”. Given a context where a single T and a single S are relevant, we may simply refer to this set as H.

How does this help us learn a better structure solver? To answer this we will need some additional notation to formally refer to parts of a structure solving model - which here we will denote with function notation simply as f(a, E) given some evolutionary data E and an amino acid sequence to fold a. Here, f is a simple reference to the complex end-to-end trainable system for structure solving described in previous sections. The function f is a neural network with many sub-modules and its parameters are learned through many rounds of optimization - where the parameterized form of f at iteration i may be denoted fi.

How does this help us? We may compute gradients of the parameters of fi given a loss that is computed by comparing predictions of H (i.e. hSt) to the experimentally obtained values of H. Such a loss can be some form of normalized sum over the differences in predictions and ground truths for each individual target such as [1]. It is unclear at this point whether normalizing each hStbefore combining in such a sum would be of benefit.

LH = tT(hSt-hSt)						[1]

Suppose further that we have a loss that can score the model on its concurrence in some way with known crystallographic structures LC. We posit that the inclusion of H and the addition, in some form, of LH, will accomplish an acceleration of the minimization of LC or, equivalently, a significant increase in the data efficiency of the minimization of LC for which crystallography data is generally considered expensive and scarce. In plain english, we posit that by including a loss term in the form of LH, or even simpler by considering the model’s ability to recapitulate H, we will be able to learn a structure solver that arrives at 3D structures that are more accurate. Or even more simply, by considering H in the optimization of our structure solver we will be able to learn a better one.

This paradigm could be extended to any means of constructing S - not only by way of SELEX. We could for example use an existing version of a model to predict the binding affinity of a set of oligos and optimize these for their diversity and useful resolution. We could then synthesize that set of oligos and use it to construct an H that is used to train another version of the model. Thereby making rational choices about the informativeness of H. While interesting, it seems constructing a sufficiently diverse S* as discussed above would be sufficient.

Learning curricula and data augmentation in the context of holography data

One natural source of data augmentation to apply in this context would be the addition of noise or dropout to the target spectrum. A form of learning curricula would be to subset an H dataset by the size of elements of T and initially training models only on the smallest targets followed by progressive increases in size.

Array-recovery holography

The above discussion elaborated on the merits of having a dataset H of the affinity of sequences S over targets T towards informing the learning of models able to implicitly learn about the structure of those targets based on the sequences with which they tend to bind. One way to present the target set for binding (including for SELEX to develop the probe set S) would be in microarray format. Figure 5 describes such a method. In short, following binding, the sequence of each oligo may be related to its array location of origin through multiple means. One, as diagrammed in the figure, involved aliquoting each microarray spot and barcoding within aliquots. Another would be to simply attach barcodes to oligos on the surface of the microarray by depositing a microdroplet of barcode and necessary enzymes to each microarray spot. In the service of this, microarray binding spots may be separated from each-other by a hydrophobic region. Naturally this method differs from SELEX itself in terms of the desired output and use of that - where individual high-affinity probes are not sought but rather a full spectral profile of binding affinities across many probes, however high or low in affinity. One important step in estimating the abundance of sequences bound to targets would be with respect to a baseline of abundances of sequences in the input set. The curriculum approach described above could be achieved here given protein identities are known.


Figure 5. Overview diagram of array-recovery holography. I. As in conventional SELEX methods, a set of input oligos (A) is permitted to bind to a set of targets (B) which in this case are present in spots at known positions on an array or other fixed substrate. Unbound oligos are discarded and bound oligos are released and collected (C), becoming the input (A) for a subsequent round of array-based SELEX. Instead of performing SELEX in this way, such a binding probe set could be obtained via SELEX in another context or may simply be a random or computationally-designed set of oligos that was not obtained via SELEX. SELEX may be performed within droplets by washing away unbound followed by reducing volume to separate spots with hydrophobic regions and amplifying within spots. II. Array spots are then sampled and the polymers of interest in each (e.g. DNA oligos) are barcoded and pooled for NGS sequencing. 
Free target holography

Figure 6 details a method for binding of free targets (e.g. cell lysate after protein purification and reconstitution) by a set of oligo probes such as those derived via SELEX - whether performed for these free targets or previously against a differently-obtained set of oligo structure probes. The result is a set of aliquot pairs (protein and DNA). DNA sequence is easily determined by sequencing and related back to the pair by way of the barcode. Protein identity can be determined through a variety of means including by way of de novo protein sequencing, in relation to a reference ladder spiked in during the separation step, using machine learning methods to predict protein identity from mass spectrometry data (using a database of proteins and corresponding mass spectrometry profiles for training), or by applying other standard methods of protein identification. If the chromatography or other separation step were to separate proteins by their size this could be used to implement a learning curriculum described above.


Figure 6. A simplified methodological illustration for free target holography. Bind and crosslink. Oligos (A, B) are amplified according to their affinity for protein targets. Oligos are fixed to their targets by cross-linking. Separate, fraction, and barcode. Target-oligo complexes are separated by chromatography or other means of size, charge, and/or other affinity separation, producing fractions that contain distinct types of complexes. These are further fractioned into paired aliquots and for the DNA fractions, barcoded (numbers). Sequence and identify. DNA sequencing and protein identification is performed on paired aliquots. Protein identification may be performed by mass spectroscopic profiling, de novo sequencing, or other methods. The result is an H dataset as described above, i.e. spectra of the abundance of sequences over targets. A simplified Venn diagram is shown from the perspective of the model - that, as they manifest in the wild (e.g. as they fold or behave dynamically), triangles and circles appear distinct in their geometry whereas hearts appear to share features from each.
ChIP-Seq and partial holography by natural DNA binding

Chromatin immunoprecipitation sequencing requires antibodies to the protein of interest and would be infeasible to perform at the scale required to construct a useful training dataset if each purification is to be performed individually. If a microarray of antibodies for all or many of the proteins in a genome were available, related to the previous, this might be of interest. Otherwise, ChIP-seq will be an interesting way to begin to explore the usefulness of the method (e.g. using all existing ChIP-seq datasets) but we anticipate the generality of this is limited to the binding conditions particular to the experiments from which these data were derived (largely the binding patterns of DNA regulatory proteins). Nevertheless, such data would be expected to provide at least some signal regarding the 3D shape of proteins although would be biased towards simply predicting DNA recognition motifs.

Protein structure capture by high-throughput sequencing (pHi-C)

One might generate pairwise distance matrices encoding the structure of individual proteins using a method similar to Hi-C whereby pairs of DNA strands are cross-linked to the proteins to which they are bound, end-ligated with a biotinylated nucleotide, separated, purified, and sequenced. This may be applied in natural contexts but we imagine more informative information would result from the application of a set of sequences first evolved through a SELEX-like procedure to tend to have affinity for proteins in the mixture. In the service of even coverage of affinity oligo 

Interactome data

Protein-protein, protein-compound, and other types of interactomes have within them information about similarities in the structures of interacting entities. A model trained to make such predictions may likely learn useful representations for a downstream or concurrent task (such as the prediction of a distance matrix as in Senior et al. (2019) and others). The human protein-protein interactome (HuRI; Luck et al., 2019) may be used for such a purpose. This dataset, when combined with all previously published datasets, includes 64,006 binary protein-protein interactions (PPI) among 9,094 proteins within the total space of 82.7M possible interactions. When training a model on this dataset the set of possible training examples is far larger than just those positive PPI pairs but rather can include any amount of randomly sampled non-interacting pairs which is a process known as “negative sampling”. The same might apply if using a protein-compound dataset such as BIOSNAP-DTI (where DTI is an abbreviation for drug-target interaction) which contains 5,018 drugs and 2,325 proteins with a total of 15,139 drug-target interactions.

There would be various ways to write a loss to learn a model from such data. One might be to seek to simply predict whether a given pair of proteins or compounds do or do not interact. Another approach might be to pre-process a similarity measure between all pairs of proteins according to their interaction profile with the larger set and, given that pair as input, train the model to predict that computed similarity score. The latter may be of interest to emphasize the importance of representing similar structures and behaviors in a similar way. 

There are additional reasons why a biophysical simulator might benefit from being trained on datasets of protein-protein or protein-compound interactions beyond the concern of inferring the 3D structure of single proteins - that being to develop an implicit understanding of these other kinds of activities that may become a basis for transfer learning on subsequent tasks that require such knowledge. For example, we might seek to first infer the 3D structure of one or many proteins in the human genome followed by performing a high-throughput digital screening of candidate compounds that will interact with certain target proteins in a desired way while not interacting with many or any others (as is a core means of attaining both efficacy and safety of pharmaceutical treatments).

There are other means of inferring interactome data in a manner less labor-intensive than that employed by Luck et al. (2020; albeit perhaps less precisely informative). One approach is to infer interactions from correlations in gene expression datasets - of which there is likely more than several petabytes freely available for research use.

Another closely-related type of dataset that may be used in a similar way would be protein antibody arrays. For example, Wang et al. (2020) report the construction of a 60,000 antibody array that can be used, for their purposes, for “antibody development and target discovery”. Such a system, just as well for smaller-scale protein antibody arrays, could be used for the purpose of holography as described above for protein interactomes. Here, by learning to predict which proteins will be recognized by which antibodies (and to what extent) a model would learn implicitly about the folded shape of those proteins - and antibodies, perhaps a further useful pre-training task for a downstream application of the computational design of sensitive and specific antibodies.

In this context as in others, data augmentation would enable more to be gained from a fixed-size dataset. The learning problems as described above for PPI or PCI may be augmented with noise added to the binary label of interaction or to the continuous value of interaction similarity or strength. 

Compound property prediction as pre-training

As discussed above, a promising direction is to use protein-compound interactions to learn about the structure of proteins. A useful pre-training step to precede this would be to teach networks to understand the properties of compounds themselves. There are various compound property prediction datasets available.